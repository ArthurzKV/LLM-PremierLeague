# HISTORIA DE LOS LLM
El desarrollo de los Modelos de Lenguaje de Gran Tamaño (LLM) es el resultado de décadas de avances en el campo del procesamiento del lenguaje natural (PLN) y el aprendizaje automático. Aunque los LLM son una innovación relativamente reciente, sus raíces se remontan a los primeros intentos por enseñar a las máquinas a comprender el lenguaje humano.
En las décadas de 1950 y 1960, los primeros enfoques del PLN se basaban en reglas gramaticales explícitas y estructuras lingüísticas formales, pero eran muy limitados. Posteriormente, en los años 80 y 90, surgieron enfoques estadísticos que utilizaban grandes corpus de texto para estimar la probabilidad de aparición de palabras. Estos modelos n-gram se convirtieron en una base importante para tareas como la traducción automática y el análisis de texto.
El verdadero cambio comenzó con el auge del aprendizaje profundo. En 2013, Word2Vec (de Google) introdujo una forma eficiente de representar palabras en vectores numéricos (embeddings), lo que permitió a las máquinas captar relaciones semánticas entre palabras. Esto fue seguido por modelos como ELMo (2018) y ULMFiT, que introdujeron el concepto de contextualización: es decir, una palabra cambia de significado según el contexto en que aparece.
El punto de inflexión llegó con la aparición del modelo Transformer en 2017, propuesto por Vaswani et al. en el paper "Attention is All You Need". Esta arquitectura permitió entrenar modelos de lenguaje de forma más eficiente y con mayor capacidad para capturar relaciones a largo plazo entre palabras. A partir de ahí, surgieron 
# los primeros LLM a gran escala:

BERT (2018) de Google, especializado en tareas de comprensión de texto.
GPT (2018, 2019, 2020, 2023) por OpenAI, que introdujo la generación de texto con una calidad sin precedentes.
T5, XLNet, RoBERTa, PaLM, LLaMA, Claude, Gemini, entre otros, desarrollados por distintas organizaciones tecnológicas.


Con cada generación, los LLM han aumentado su número de parámetros (de millones a cientos de miles de millones), lo que ha ampliado sus capacidades de comprensión, generación y razonamiento.

Hoy en día, los LLM son la base de tecnologías de asistentes conversacionales, motores de búsqueda inteligentes, herramientas de programación automática y mucho más, y continúan evolucionando con investigaciones enfocadas en eficiencia, ética, control, transparencia y personalización.